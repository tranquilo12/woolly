# ğŸš€ Fresh Start: Phase 4 - Advanced Features & Caching Layer

## ğŸ“‹ **TLDR - Next Phase Priority**

**Status:** âœ… Phase 2-3 Complete | ğŸ¯ **Starting Phase 4 - Advanced Features & Caching Layer**
**Context:** Backend Simplification Plan - Universal Agent Architecture
**Achievement:** 75% code reduction, Universal system operational, All endpoints working
**Next Goal:** Implement streaming responses, agent chaining, caching layer, and performance monitoring

---

## ğŸ¯ **MISSION: Phase 4 Implementation**

You are continuing the **Backend Simplification Plan** for the Woolly project. **Phase 2-3 has been completed successfully** with major achievements:

- âœ… **75% code reduction** (1,400 â†’ 350 lines core logic)
- âœ… **Universal Agent System** fully operational with 5 agent types
- âœ… **Legacy cleanup** completed with backward compatibility
- âœ… **FastAPI route conflicts** resolved
- âœ… **Pydantic AI best practices** implemented throughout

**Your task:** Implement **Phase 4: Advanced Features & Caching Layer** following the established patterns and maintaining the simplified architecture.

---

## ğŸ“Š **CURRENT SYSTEM STATE**

### âœ… **Working Components (DO NOT MODIFY)**

```
âœ… OPERATIONAL ENDPOINTS:
- GET  /api/health                    (Main health check)
- GET  /api/agents/health            (Agent system health)
- GET  /api/agents                   (List all agents)
- POST /api/agents                   (Create agent)
- GET  /api/agents/{id}              (Get specific agent)
- PUT  /api/agents/{id}              (Update agent)
- DELETE /api/agents/{id}            (Delete agent)

- GET  /api/v1/agents/types          (Available agent types)
- POST /api/v1/agents/execute        (Universal agent execution)
- GET  /api/v1/agents/status/{id}    (Task status)

âš ï¸  DEPRECATED (but working):
- POST /api/generate/{specialization} (Legacy endpoint with warnings)
```

### ğŸ—ï¸ **Current Architecture**

```
api/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ universal.py          âœ… Universal Agent Factory (working)
â”‚   â””â”€â”€ parallel.py           âœ… Parallel execution (working)
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ agents.py            âœ… Legacy router (270 lines, simplified)
â”‚   â”œâ”€â”€ universal_agents.py  âœ… Universal router (452 lines, working)
â”‚   â””â”€â”€ documentation.py     âœ… Documentation router (60 lines)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ models.py            âœ… Universal models (working)
â”‚   â”œâ”€â”€ database.py          âœ… Database operations (working)
â”‚   â””â”€â”€ openai_client.py     âœ… OpenAI client (working)
â””â”€â”€ index.py                 âœ… Main app (simplified, working)
```

---

## ğŸ¯ **PHASE 4 OBJECTIVES**

### **Priority 1: Streaming Responses**

**Goal:** Real-time agent output streaming for better UX
**Files to Create/Modify:**

- `api/routers/universal_agents.py` - Add streaming endpoints
- `api/agents/streaming.py` - New streaming manager
- `api/utils/streaming.py` - Streaming utilities

### **Priority 2: Agent Chaining**

**Goal:** Sequential agent execution with context passing
**Files to Create/Modify:**

- `api/agents/chaining.py` - Agent chain orchestrator
- `api/utils/models.py` - Add chain models
- `api/routers/universal_agents.py` - Add chaining endpoints

### **Priority 3: Caching Layer (from Phase 5)**

**Goal:** Redis-based result caching for performance
**Files to Create/Modify:**

- `api/utils/cache.py` - Redis caching utilities
- `api/agents/universal.py` - Add caching to factory
- `requirements.txt` - Add Redis dependencies

### **Priority 4: Performance Monitoring**

**Goal:** Advanced metrics and observability
**Files to Create/Modify:**

- `api/utils/monitoring.py` - Metrics collection
- `api/routers/monitoring.py` - Monitoring endpoints
- `api/middleware/metrics.py` - Request/response metrics

---

## ğŸ—ï¸ **ARCHITECTURAL GUIDANCE**

### **1. Maintain Universal Patterns**

```python
# âœ… CONTINUE USING: Universal models for all new features
class StreamingRequest(BaseModel):
    """Extend universal patterns for streaming"""
    repository_name: str
    agent_type: AgentType
    user_query: str
    stream_mode: bool = True
    context: Dict[str, Any] = Field(default_factory=dict)

class ChainRequest(BaseModel):
    """Extend universal patterns for chaining"""
    repository_name: str
    agent_chain: List[AgentType]
    user_query: str
    context: Dict[str, Any] = Field(default_factory=dict)
```

### **2. Follow Pydantic AI Best Practices**

```python
# âœ… CONTINUE PATTERN: Single factory with extensions
class UniversalAgentFactory:
    def __init__(self):
        self.mcp_server = MCPServerSSE(url="http://localhost:8009/sse")
        self.cache = RedisCache()  # NEW: Add caching
        self.streaming_manager = StreamingManager()  # NEW: Add streaming

    async def create_streaming_agent(self, agent_type: AgentType) -> Agent:
        """NEW: Create agent with streaming capabilities"""
        # Follow existing patterns

    async def create_chain_agents(self, chain: List[AgentType]) -> List[Agent]:
        """NEW: Create agent chain with context passing"""
        # Follow existing patterns
```

### **3. Extend Router Patterns**

```python
# âœ… EXTEND: Universal router with new endpoints
@router.post("/agents/stream")
async def stream_agent_response(request: StreamingRequest):
    """NEW: Streaming endpoint following universal patterns"""

@router.post("/agents/chain")
async def execute_agent_chain(request: ChainRequest):
    """NEW: Chain endpoint following universal patterns"""
```

---

## ğŸ”§ **IMPLEMENTATION SPECIFICATIONS**

### **1. Streaming Implementation**

```python
# File: api/agents/streaming.py
from typing import AsyncGenerator
from fastapi.responses import StreamingResponse

class StreamingManager:
    """Manage real-time agent output streaming"""

    async def stream_agent_response(
        self,
        agent: Agent,
        query: str,
        deps: UniversalDependencies
    ) -> AsyncGenerator[str, None]:
        """Stream agent responses in real-time"""
        async with agent.run_stream(query, deps=deps) as result:
            async for chunk in result.stream():
                yield f"data: {json.dumps({'content': chunk, 'type': 'chunk'})}\n\n"

        yield f"data: {json.dumps({'type': 'complete'})}\n\n"

# File: api/routers/universal_agents.py (extend existing)
@router.post("/agents/stream")
async def stream_agent_execution(request: StreamingRequest):
    """Stream agent responses in real-time"""
    agent = universal_factory.create_agent(request.agent_type)
    deps = UniversalDependencies(
        repository_name=request.repository_name,
        agent_type=request.agent_type,
        user_query=request.user_query,
        context=request.context
    )

    return StreamingResponse(
        streaming_manager.stream_agent_response(agent, request.user_query, deps),
        media_type="text/event-stream"
    )
```

### **2. Agent Chaining Implementation**

```python
# File: api/agents/chaining.py
class AgentChainOrchestrator:
    """Orchestrate sequential agent execution with context passing"""

    def __init__(self):
        self.factory = universal_factory

    async def execute_chain(
        self,
        agent_chain: List[AgentType],
        initial_query: str,
        repository_name: str,
        context: Dict[str, Any] = None
    ) -> ChainResult:
        """Execute agents in sequence, passing context between them"""
        results = []
        current_context = context or {}

        for i, agent_type in enumerate(agent_chain):
            # Create agent with accumulated context
            agent = self.factory.create_agent(agent_type)

            # Build query with previous results
            if i > 0:
                enhanced_query = f"""
                Previous agent results: {json.dumps(results[-1])}

                Current task: {initial_query}

                Continue the analysis building on the previous results.
                """
            else:
                enhanced_query = initial_query

            # Execute agent
            deps = UniversalDependencies(
                repository_name=repository_name,
                agent_type=agent_type,
                user_query=enhanced_query,
                context=current_context
            )

            result = await agent.run(enhanced_query, deps=deps)
            results.append(result.data)

            # Update context for next agent
            current_context.update({
                f"agent_{i}_result": result.data,
                f"agent_{i}_type": agent_type.value
            })

        return ChainResult(
            chain=agent_chain,
            results=results,
            final_context=current_context
        )
```

### **3. Caching Layer Implementation**

```python
# File: api/utils/cache.py
import redis
import json
import hashlib
from typing import Optional, Any

class RedisCache:
    """Redis-based caching for agent results"""

    def __init__(self):
        self.redis_client = redis.Redis(
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", 6379)),
            db=0,
            decode_responses=True
        )

    def _generate_cache_key(
        self,
        agent_type: AgentType,
        query: str,
        repository_name: str,
        context: Dict[str, Any]
    ) -> str:
        """Generate consistent cache key"""
        cache_data = {
            "agent_type": agent_type.value,
            "query": query,
            "repository": repository_name,
            "context": context
        }
        cache_string = json.dumps(cache_data, sort_keys=True)
        return f"agent_result:{hashlib.md5(cache_string.encode()).hexdigest()}"

    async def get_cached_result(
        self,
        agent_type: AgentType,
        query: str,
        repository_name: str,
        context: Dict[str, Any]
    ) -> Optional[UniversalResult]:
        """Get cached agent result"""
        cache_key = self._generate_cache_key(agent_type, query, repository_name, context)
        cached_data = self.redis_client.get(cache_key)

        if cached_data:
            return UniversalResult.model_validate_json(cached_data)
        return None

    async def cache_result(
        self,
        agent_type: AgentType,
        query: str,
        repository_name: str,
        context: Dict[str, Any],
        result: UniversalResult,
        ttl: int = 3600  # 1 hour default
    ):
        """Cache agent result"""
        cache_key = self._generate_cache_key(agent_type, query, repository_name, context)
        self.redis_client.setex(
            cache_key,
            ttl,
            result.model_dump_json()
        )

# File: api/agents/universal.py (extend existing)
class UniversalAgentFactory:
    def __init__(self):
        self.mcp_server = MCPServerSSE(url="http://localhost:8009/sse")
        self.cache = RedisCache()  # NEW: Add caching

    async def execute_with_cache(
        self,
        agent_type: AgentType,
        query: str,
        repository_name: str,
        context: Dict[str, Any] = None
    ) -> UniversalResult:
        """Execute agent with caching"""
        # Check cache first
        cached_result = await self.cache.get_cached_result(
            agent_type, query, repository_name, context or {}
        )

        if cached_result:
            logger.info(f"Cache hit for {agent_type.value} query")
            return cached_result

        # Execute agent
        agent = self.create_agent(agent_type)
        deps = UniversalDependencies(
            repository_name=repository_name,
            agent_type=agent_type,
            user_query=query,
            context=context or {}
        )

        result = await agent.run(query, deps=deps)

        # Cache result
        await self.cache.cache_result(
            agent_type, query, repository_name, context or {}, result.data
        )

        return result.data
```

### **4. Performance Monitoring Implementation**

```python
# File: api/utils/monitoring.py
import time
import psutil
from prometheus_client import Counter, Histogram, Gauge
from typing import Dict, Any

# Metrics
REQUEST_COUNT = Counter('agent_requests_total', 'Total agent requests', ['agent_type', 'status'])
REQUEST_DURATION = Histogram('agent_request_duration_seconds', 'Agent request duration', ['agent_type'])
ACTIVE_AGENTS = Gauge('active_agents', 'Number of active agents')
CACHE_HITS = Counter('cache_hits_total', 'Total cache hits', ['agent_type'])
CACHE_MISSES = Counter('cache_misses_total', 'Total cache misses', ['agent_type'])

class PerformanceMonitor:
    """Monitor agent performance and system metrics"""

    @staticmethod
    def record_request(agent_type: AgentType, duration: float, status: str):
        """Record agent request metrics"""
        REQUEST_COUNT.labels(agent_type=agent_type.value, status=status).inc()
        REQUEST_DURATION.labels(agent_type=agent_type.value).observe(duration)

    @staticmethod
    def record_cache_hit(agent_type: AgentType):
        """Record cache hit"""
        CACHE_HITS.labels(agent_type=agent_type.value).inc()

    @staticmethod
    def record_cache_miss(agent_type: AgentType):
        """Record cache miss"""
        CACHE_MISSES.labels(agent_type=agent_type.value).inc()

    @staticmethod
    def get_system_metrics() -> Dict[str, Any]:
        """Get current system metrics"""
        return {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "active_agents": ACTIVE_AGENTS._value._value
        }

# File: api/routers/monitoring.py
from fastapi import APIRouter
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST

router = APIRouter()

@router.get("/metrics")
async def get_metrics():
    """Prometheus metrics endpoint"""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

@router.get("/health/detailed")
async def detailed_health():
    """Detailed health check with system metrics"""
    return {
        "status": "healthy",
        "system_metrics": PerformanceMonitor.get_system_metrics(),
        "agent_factory": "operational",
        "cache": "operational" if redis_client.ping() else "unavailable",
        "mcp_server": "operational"
    }
```

---

## ğŸ“‹ **TODO LIST FOR NEXT CONVERSATION**

### **ğŸ¯ Phase 4 Implementation Tasks**

**Priority 1: Streaming Responses**

- [ ] Create `api/agents/streaming.py` with StreamingManager class
- [ ] Add streaming endpoints to `api/routers/universal_agents.py`
- [ ] Create `api/utils/streaming.py` for streaming utilities
- [ ] Add StreamingRequest and StreamingResponse models to `api/utils/models.py`
- [ ] Test streaming endpoints with real-time output

**Priority 2: Agent Chaining**

- [ ] Create `api/agents/chaining.py` with AgentChainOrchestrator
- [ ] Add ChainRequest and ChainResult models to `api/utils/models.py`
- [ ] Add chain execution endpoint to `api/routers/universal_agents.py`
- [ ] Implement context passing between agents
- [ ] Test multi-agent chains with different combinations

**Priority 3: Caching Layer**

- [ ] Create `api/utils/cache.py` with RedisCache class
- [ ] Add Redis dependencies to `requirements.txt`
- [ ] Extend UniversalAgentFactory with caching capabilities
- [ ] Add cache configuration to environment variables
- [ ] Test cache hit/miss scenarios

**Priority 4: Performance Monitoring**

- [ ] Create `api/utils/monitoring.py` with PerformanceMonitor
- [ ] Create `api/routers/monitoring.py` with metrics endpoints
- [ ] Add Prometheus client dependencies to `requirements.txt`
- [ ] Create `api/middleware/metrics.py` for request/response metrics
- [ ] Test metrics collection and endpoint exposure

**Priority 5: Integration & Testing**

- [ ] Update `api/index.py` to include new routers
- [ ] Add environment variable configuration for Redis and monitoring
- [ ] Create integration tests for all new features
- [ ] Update documentation with new endpoints
- [ ] Performance test new features vs baseline

---

## âš ï¸ **CRITICAL GUIDELINES**

### **ğŸš¨ DO NOT MODIFY EXISTING WORKING COMPONENTS**

- âœ… Keep `api/agents/universal.py` working patterns
- âœ… Keep `api/routers/universal_agents.py` existing endpoints
- âœ… Keep `api/utils/models.py` universal models
- âœ… Keep all working endpoints functional

### **ğŸ¯ FOLLOW ESTABLISHED PATTERNS**

- âœ… Use UniversalDependencies/UniversalResult patterns
- âœ… Follow Pydantic AI best practices
- âœ… Maintain type safety throughout
- âœ… Use async/await for all operations
- âœ… Follow DRY principles

### **ğŸ“Š MAINTAIN PERFORMANCE**

- âœ… Add caching to improve response times
- âœ… Use streaming for better UX
- âœ… Monitor performance metrics
- âœ… Optimize database queries

### **ğŸ”§ EXTEND, DON'T REPLACE**

- âœ… Add new functionality alongside existing
- âœ… Maintain backward compatibility
- âœ… Add deprecation warnings if needed
- âœ… Keep universal patterns consistent

---

## ğŸ¯ **SUCCESS CRITERIA**

### **Phase 4 Complete When:**

1. âœ… **Streaming responses** working for all agent types
2. âœ… **Agent chaining** operational with context passing
3. âœ… **Caching layer** reducing response times by 50%+
4. âœ… **Performance monitoring** providing comprehensive metrics
5. âœ… **All existing functionality** still working
6. âœ… **Integration tests** passing for new features
7. âœ… **Documentation** updated with new capabilities

### **Expected Outcomes:**

- **Improved UX**: Real-time streaming responses
- **Enhanced Capabilities**: Multi-agent workflows
- **Better Performance**: Caching reduces latency
- **Operational Visibility**: Comprehensive monitoring
- **Maintained Simplicity**: Universal patterns extended, not replaced

---

## ğŸ“š **REFERENCE FILES**

**Primary Reference:** `Backend-Simplification-Plan.md` - Complete project context
**Current Working Files:**

- `api/agents/universal.py` - Universal agent factory (extend this)
- `api/routers/universal_agents.py` - Universal router (extend this)
- `api/utils/models.py` - Universal models (extend this)
- `api/utils/database.py` - Database operations (reference this)

**Architecture Patterns:** Follow the established universal patterns documented in the Backend Simplification Plan.

---

## ğŸš€ **GET STARTED**

1. **Read the Backend Simplification Plan** to understand full context
2. **Review current working system** to understand patterns
3. **Start with Priority 1** (Streaming Responses)
4. **Create TODO list** using the provided structure
5. **Implement incrementally** following established patterns
6. **Test thoroughly** to ensure no regressions
7. **Document progress** and update metrics

**Remember:** You're extending a successful, simplified system. Follow the patterns that got us here, and maintain the elegance while adding powerful new capabilities.

ğŸ¯ **Ready to begin Phase 4!**
